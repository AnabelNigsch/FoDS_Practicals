{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 2 : Generative and Discriminative Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will compare the NaÃ¯ve Bayes Classifier (NBC) and Logistic Regression on several\n",
    "datasets. As part of the practical you should read briefly the following paper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Discriminative vs. Generative classifiers: A comparison of logistic regression\n",
    "and naive Bayes**  \n",
    "*Andrew Y. Ng and Michael I. Jordan*  \n",
    "Advances in Neural Information Processing Systems (NIPS) 2001.\n",
    "\n",
    "The paper is available on OLAT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read the Introduction and the Experiments sections. The goal of this practical is\n",
    "to qualitatively reproduce some of the experimental results in this paper. You are strongly\n",
    "encouraged to read the rest of the paper, which is rather short and straightforward to read,\n",
    "though some of you may want to skip the formal proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Bayes Classifier (NBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a NaÃ¯ve Bayes Classifier from scartch using NumPy. To keep your code tidy,\n",
    "we recommend implementing it as a class. \n",
    "The classifier should be able to handle binary and continuous features. \n",
    "To earn the bonus points, the classifier should be able to handle categorical features as well. \n",
    "Suppose the data has 3\n",
    "different features, the first being binary, the second being continuous and the third being categorical. Write an implementation that you can initialise as follows:\n",
    "\n",
    "    nbc = NBC(feature_types=['b', 'r', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the lines of classifiers provided in sklearn, you want to implement two more functions,\n",
    "**fit** and **predict**. \n",
    "Recall the joint distribution of a generative model: $p(\\mathbf{x}, y \\mid \\theta, \\pi) = p(y \\mid \\pi) \\cdot p(\\mathbf{x} \\mid y, \\theta)$.\n",
    "The **fit** function is to estimate all the parameters ($\\theta$ and $\\pi$) of the NBC, i.e., train the classifier. The **predict** function is to compute the probabilities that the new input belongs to all classes and\n",
    "then return the class that has the largest probability, i.e., make the prediction.\n",
    "\n",
    "    nbc.fit(X_train, y_train)\n",
    "    ypredicted = nbc.predict(X_test)\n",
    "    test_accuracy = np.mean(ypredicted == ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:03.564810Z",
     "start_time": "2025-11-22T15:24:02.747493Z"
    }
   },
   "source": [
    "import sys, site, pathlib\n",
    "\n",
    "print(\"sys.prefix:\", sys.prefix)\n",
    "print(\"sys.base_prefix:\", sys.base_prefix)\n",
    "print(\"in_venv:\", sys.prefix != sys.base_prefix)\n",
    "\n",
    "print(\"site-packages:\")\n",
    "for p in site.getsitepackages():\n",
    "    print(\"  \", p)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"numpy:\", np.__version__, \"->\", pathlib.Path(np.__file__).parents[1])\n",
    "print(\"pandas:\", pd.__version__, \"->\", pathlib.Path(pd.__file__).parents[1])\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10., 10.)\n",
    "\n",
    "import pickle as cp\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.prefix: /home/anabel/repositories/FoDS_Practicals/.venv\n",
      "sys.base_prefix: /usr\n",
      "in_venv: True\n",
      "site-packages:\n",
      "   /home/anabel/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages\n",
      "   /home/anabel/repositories/FoDS_Practicals/.venv/local/lib/python3.10/dist-packages\n",
      "   /home/anabel/repositories/FoDS_Practicals/.venv/lib/python3/dist-packages\n",
      "   /home/anabel/repositories/FoDS_Practicals/.venv/lib/python3.10/dist-packages\n",
      "numpy: 2.2.6 -> /home/anabel/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages\n",
      "pandas: 2.3.3 -> /home/anabel/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-conditional distributions\n",
    "\n",
    "Before implementing NBC, we first implement the class-conditional distributions $p(\\mathbf{x} \\mid y, \\theta)$. Your implementation should have two functions: **estimate** and **get_log_probability**. \n",
    "\n",
    "- The **estimate** function takes data as input and models the data using some distribution $p(x \\mid \\theta)$, where $\\theta$ is the parameters of this distribution. The function estimates the parameters $\\theta$ using maximum likelihood estimators (MLE). \n",
    "For example, in the case of continuous features, we use the Gaussian distribution to model the data. The estimate function will find the parameters $\\mu$ and $\\sigma$ for the Gaussian distribution with respect to the input data. \n",
    "\n",
    "- The **get_log_probability** function takes as input a new data point $x_{new}$ and returns the log of $p(x_{new} \\mid \\theta)$. For the Gaussian distribution, the function get_probability will return $\\mathcal{N}(x_{new} \\mid \\mu, \\sigma)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different types of features, you need to use different distributions.\n",
    "You can import statistic libraries (e.g., `scipy.stats`) for the implementation of the distributions. \n",
    "\n",
    "- For **continuous features**: Use Gaussian distribution\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\n",
    "- For **binary features**: Use Bernoulli distribution \n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html\n",
    "- For **categorical features**: Use Multinoulli distribution (The multinoulli distribution is a special case of the multinomial distribution, where the number of trials is 1)\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multinomial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Issues:**\n",
    "- The probabilities can be very small. To avoid underflow issues, you should compute the log of the probabilities. Read more: (Mur) Chapter 3.5.3 / Lecture Notes\n",
    "- The standard deviation for Gaussian distributions should never be exactly 0, so in\n",
    "case your calculated standard deviation is 0, you may want to set it to a small value such as 1e âˆ’ 6. This is to ensure that your code never encounters division by zero or\n",
    "taking logarithms of 0 errors. \n",
    "For this practical, please set the small value to 1e-6.\n",
    "- Laplace/Additive smoothing: You want to ensure that the estimates for the parameter for the Bernoulli and Multinoulli random variables is never exactly 0 or 1. For this reason you should consider using Laplace smoothing (https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "For this practical, please set alpha to 1.\n",
    "- We will check the correctness of your implementation using the tests below.\n",
    "- For simplicity, you can assume the data values for binary features are integers from {0,1} and the data for a categorical feature with M categories are integers from {0, ..., M-1}.\n",
    "- Fell free to add auxiliary functions or change the parameters of the functions. If you change the parameters of the functions, make sure you change the tests accordingly, so we can test your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:03.577106Z",
     "start_time": "2025-11-22T15:24:03.571214Z"
    }
   },
   "source": [
    "ALPHA = 1.0 #Laplace / additive smoothing parameter alpha https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "\n",
    "#Distribution for continuous features\n",
    "class ContFeatureParam:\n",
    "    def estimate(self, X):\n",
    "        #Finding the Gaussian distribution ğ’©(ğ‘¥;ğœ‡,ğœ^2) that fits the data using MLE ğœƒğ‘—ğ‘={ğœ‡,ğœ}\n",
    "\n",
    "        self.mu = X.mean()\n",
    "        sigma = X.std()\n",
    "\n",
    "        #The standard deviation for Gaussian distributions should never be exactly 0, so in case your calculated standard deviation is 0, set it to 1e-6.\n",
    "        self.sigma = max(sigma, 1e-6)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_log_probability(self, X_new):\n",
    "        #Computing ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^ğ‘— | ğ‘¦=ğ‘,ğœƒğ‘—ğ‘) using ğœƒğ‘—ğ‘: Gaussian: ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^ğ‘— | ğ‘¦=ğ‘,ğœƒğ‘—ğ‘) = ğ’©(ğ‘¥ğ‘›ğ‘’ğ‘¤^ğ‘—;ğœ‡,ğœ^2)\n",
    "        #https://en.wikipedia.org/wiki/Normal_distribution: p(x) = (1 / sqrt(2ğœ‹ğœ^2)) * exp( -(x-Î¼)^2 / (2ğœ^2)\n",
    "        #The probabilities can be very small. To avoid underflow issues, you should compute the log of the probabilities.\n",
    "        #log ğ’©(ğ‘¥;ğœ‡,ğœ^2) = -0.5 * [ log(2ğœ‹ğœ^2) + (x-Î¼)^2 / ğœ^2 ]\n",
    "\n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        var = sigma ** 2\n",
    "        log_p_x = -(0.5 * np.log(2 * np.pi * var)) -(((X_new - mu) ** 2) / (2 * var))\n",
    "\n",
    "        return log_p_x\n",
    "\n",
    "\n",
    "class BinFeatureParam:\n",
    "    def estimate(self, X):\n",
    "\n",
    "        n = X.size\n",
    "        count1 = X.sum() #number of apperance of 1\n",
    "\n",
    "        # Laplace/Additive smoothing: You want to ensure that the estimates for the parameter for the Bernoulli and Multinoulli random variables is never exactly 0 or 1. For this reason you should consider using Laplace smoothing (https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "        self.p1 = (count1 + ALPHA) / (n + (2*ALPHA)) ##There are 2 possible values (0 and 1)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_log_probability(self, X_new):\n",
    "        #Bernoulli pmf:     p(x) = p1^x * (1 - p1)^(1-x) for x âˆˆ {0,1} https://en.wikipedia.org/wiki/Bernoulli_distribution\n",
    "        #Taking logs:   log p(x) = x * log(p1) + (1 - x) * log(1 - p1) to avoid underflow\n",
    "\n",
    "        p1 = self.p1\n",
    "        p0 = 1 - p1\n",
    "\n",
    "        return X_new * np.log(p1) + (1 - X_new) * np.log(p0)\n",
    "\n",
    "\n",
    "class CatFeatureParam:\n",
    "    def __init__(self, num_of_categories):\n",
    "        self._num_of_categories = num_of_categories\n",
    "\n",
    "    def estimate(self, X):\n",
    "        #Estimation: ğœƒğ‘—ğ‘ = {ğ‘(ğ‘¥=ğ‘–),...(ğ‘¥=ğ‘˜)}\n",
    "        #Prediction:  ğ‘(ğ‘– | ğœƒğ‘—ğ‘) = ğ‘(ğ‘¥ = ğ‘–)\n",
    "        k = self._num_of_categories\n",
    "        n = X.size\n",
    "\n",
    "        #k distinct values in j-th column of the data where x âˆˆ {1, ..., k} and p(x = i) = # of apperance of i / # of data\n",
    "        #counts[i] = number of times category i appears among the samples\n",
    "        counts = np.bincount(X, minlength=k) #https://www.geeksforgeeks.org/python/numpy-bincount-python/\n",
    "\n",
    "        # Laplace/Additive smoothing: You want to ensure that the estimates for the parameter for the Bernoulli and Multinoulli random variables is never exactly 0 or 1. For this reason you should consider using Laplace smoothing (https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "        self.probs = (counts + ALPHA) / (n +(k*ALPHA)) #(k categories)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_log_probability(self, X_new):\n",
    "        log_probs = np.log(self.probs)\n",
    "        return log_probs[X_new]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests:**\n",
    "    \n",
    "We will use the code below to test the correctness of your code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:03.692893Z",
     "start_time": "2025-11-22T15:24:03.689805Z"
    }
   },
   "source": [
    "# continuous features\n",
    "\n",
    "X = np.array([2.70508547,2.10499698,1.76019132,3.42016431,3.47037973,3.67435061,1.84749286,4.3388506,2.27818252,4.65165335])\n",
    "\n",
    "param = ContFeatureParam()\n",
    "param.estimate(X)\n",
    "probs = param.get_log_probability(np.array([0,1,2,3]))\n",
    "print(probs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.64670664 -3.02757918 -1.44567455 -0.90099277]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:03.769343Z",
     "start_time": "2025-11-22T15:24:03.765499Z"
    }
   },
   "source": [
    "# binary features\n",
    "\n",
    "X = np.array([0,0,1,1,0,1,0,1,1,1])\n",
    "\n",
    "param = BinFeatureParam()\n",
    "param.estimate(X)\n",
    "probs = param.get_log_probability(np.array([0,1]))\n",
    "print(probs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.87546874 -0.5389965 ]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:03.825386Z",
     "start_time": "2025-11-22T15:24:03.821121Z"
    }
   },
   "source": [
    "# categorical features (bonus task)\n",
    "\n",
    "X = np.array([0,6,5,4,0,6,6,4,1,1,2,3,8,8,1,6,4,9,0,2,2,3,8,0,2])\n",
    "\n",
    "param = CatFeatureParam(num_of_categories=10)\n",
    "param.estimate(X)\n",
    "probs = param.get_log_probability(np.array([0,1,2,3,4,5,6,7,8,9]))\n",
    "print(probs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.94591015 -2.1690537  -1.94591015 -2.45673577 -2.1690537  -2.86220088\n",
      " -1.94591015 -3.55534806 -2.1690537  -2.86220088]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement NBC\n",
    "\n",
    "We are now ready to implement NBC. We follow the structure of models in scikit-learn. We implement NBC as a class with functions **init**, **fit** and **predict**.\n",
    "The **init** function takes as input the types of features and initialise the classifier. The **fit** function takes the training data as input and estimates the parameters. The **predict** function predicts the label for the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Issues:**\n",
    "- You should use matrix operations rather than loops. In general, loops over classes or features are OK, but loops over the rows of data are not a good idea.\n",
    "- The probabilities can be very small. To avoid underflow issues, you should do the calculations in log space. Read more: (Mur) Chapter 3.5.3 / Lecture Note\n",
    "- For simplicity, you can assume the data values for binary features are integers from {0, 1} and the data for a categorical feature with M categories are integers from {0, ..., M-1}.\n",
    "- Fell free to add auxiliary functions or change the parameters of the functions. If you change the parameters of the functions, make sure you change the tests accordingly, so we can test your code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:03.898932Z",
     "start_time": "2025-11-22T15:24:03.890029Z"
    }
   },
   "source": [
    "# Your task is to implement the three functions of NBC. \n",
    "\n",
    "class NBC:\n",
    "    # Inputs:\n",
    "    #   feature_types: the array of the types of the features, e.g., feature_types=['b', 'r', 'c']\n",
    "    def __init__(self, feature_types=[]):\n",
    "        # We implement NBC as a class with functions **init**, **fit** and **predict**. The **init** function takes as input the types of features and initialise the classifier.\n",
    "        self.feature_types = feature_types\n",
    "\n",
    "        self.classes = None # set of class labels (e.g. [0,1] or [Beyonce,Borat,KanyeWest])\n",
    "        self.log_class_prior = None # Class prior p(y = c | ğœ‹)\n",
    "        self.thetas_class_feature = None # Î¸ = {Î¸_jc | for each class c and each feature j}\n",
    "        self.num_categories = None # data for a categorical feature with M categories are integers from {0, ..., M-1}\n",
    "\n",
    "        \n",
    "    # The function uses the input data to estimate all the parameters of the NBC\n",
    "    def fit(self, X, y):\n",
    "        # fit function: Estimates all parameters\n",
    "        #   ğœ‹={ğœ‹1,â€¦,ğœ‹ğ¶}\n",
    "        #   ğœ½={ğœƒğ‘—ğ‘ |for each class ğ‘ and each feature ğ‘—}\n",
    "        # The **fit** function takes the training data as input and estimates the parameters.\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if len(self.feature_types) != n_features:\n",
    "            raise Exception(\"Number of feature types must match number of columns in X\")\n",
    "\n",
    "        # find classes and map y to indices 0, ..., C-1\n",
    "        self.classes, y_indices = np.unique(y, return_inverse=True)\n",
    "        classes_size = self.classes.size\n",
    "\n",
    "        #ğ…c= p(y = c) = probability ğœ‹ğ‘ for each class ğ‘ = # of appearance of class c / # of data\n",
    "        #Class prior: ğ‘(ğ‘¦=ğ‘ | ğ…)\n",
    "        class_counts = np.bincount(y_indices, minlength=classes_size)\n",
    "        self.log_class_prior = np.log(class_counts / n_samples)\n",
    "\n",
    "        # For simplicity, you can assume the data values for binary features are integers from {0, 1} and the data for a categorical feature with M categories are integers from {0, ..., M-1}\n",
    "        self.num_categories = [None for _ in range(n_features)]\n",
    "        for featureindex, featuretype in enumerate(self.feature_types):\n",
    "            if featuretype == 'c':\n",
    "                # categorical feature with M categories are integers from {0, ..., M-1}\n",
    "                self.num_categories[featureindex] = int(X[:, featureindex].max()) + 1\n",
    "\n",
    "        # Î¸ = {Î¸_jc | for each class c and each feature j}\n",
    "        self.thetas_class_feature = [[None for _ in range(n_features)] for _ in range(classes_size)]\n",
    "\n",
    "        # You should use matrix operations rather than loops. In general, loops over classes or features are OK, but loops over the rows of data are not a good idea.\n",
    "\n",
    "        \"\"\"\n",
    "        Pseudocode: fit\n",
    "\n",
    "        function fit(X_train, y_train):\n",
    "            for each class c:\n",
    "\n",
    "                // estimate class prior\n",
    "                pi_c <- p(y=c)\n",
    "\n",
    "                for each feature j:\n",
    "\n",
    "                    // get the data with class c and feature j\n",
    "                    X_jc <- X_train[y_train==c, j]\n",
    "\n",
    "                    // estimate theta_jc\n",
    "                    // the estimation should be based on the type of j\n",
    "                    theta_jc <- estimate theta_jc on X_jc\n",
    "        \"\"\"\n",
    "\n",
    "        for classindex, classlabel in enumerate(self.classes):\n",
    "            \"\"\"// estimate class prior\"\"\"\n",
    "            X_classlabel = X[y == classlabel]\n",
    "\n",
    "            for featureindex, featuretype in enumerate(self.feature_types):\n",
    "                \"\"\"// get the data with class c and feature j\"\"\"\n",
    "                X_classlabel_feature = X_classlabel[:, featureindex] # data used to estimate theta_jc\n",
    "\n",
    "                \"\"\"\n",
    "                // estimate theta_jc\n",
    "                // the estimation should be based on the type of j\n",
    "                theta_jc <- estimate theta_jc on X_jc\n",
    "                \"\"\"\n",
    "                if featuretype == 'r':\n",
    "                    thetas_class_feature = ContFeatureParam().estimate(X_classlabel_feature)\n",
    "                elif featuretype == 'b':\n",
    "                    thetas_class_feature = BinFeatureParam().estimate(X_classlabel_feature)\n",
    "                elif featuretype == 'c':\n",
    "                    k = self.num_categories[featureindex]\n",
    "                    thetas_class_feature = CatFeatureParam(num_of_categories=k).estimate(X_classlabel_feature)\n",
    "                else:\n",
    "                    raise Exception(f\"Unknown feature type\")\n",
    "\n",
    "                self.thetas_class_feature[classindex][featureindex] = thetas_class_feature\n",
    "\n",
    "        return self\n",
    "                \n",
    "                \n",
    "    # The function takes the data X as input, and predicts the class for the data\n",
    "    def predict(self, X):\n",
    "        # The **predict** function predicts the label for the input data.\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        class_len = len(self.classes)\n",
    "\n",
    "        #The probabilities can be very small. To avoid underflow issues, you should do the calculations in log space. Read more: (Mur) Chapter 3.5.3 / Lecture Note\n",
    "        #predict function: For a new data ğ‘¥ğ‘›ğ‘’ğ‘¤, computes for each class c:\n",
    "        #ğ‘(ğ‘¦=ğ‘|ğ‘¥ğ‘›ğ‘’ğ‘¤,ğœ½,ğ…) = (ğ‘(ğ‘¦=ğ‘|ğ…)â‹…ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤|ğ‘¦=ğ‘,ğœ½))/(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤|ğœ½,ğ…))\n",
    "        #                = (ğœ‹ğ‘*Î ğ‘—(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^j| ğ‘¦ =ğ‘,ğœƒğ‘—ğ‘))/(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤|ğœ½,ğ…))\n",
    "        #log(ğ‘(ğ‘¦=ğ‘|ğ‘¥ğ‘›ğ‘’ğ‘¤,ğœ½,ğ…)) = log(ğœ‹ğ‘)+âˆ‘ğ‘—(log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^j| ğ‘¦ =ğ‘,ğœƒğ‘—ğ‘)))-log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤|ğœ½,ğ…))\n",
    "        #Choose the class with largest probability (no need to compute the denominator -log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤|ğœ½,ğ…)) as it is same for all classes)\n",
    "\n",
    "        \"\"\"\n",
    "        Pseudocode: predict\n",
    "\n",
    "        function predict(x_new):\n",
    "\n",
    "            for each class c:\n",
    "\n",
    "                prob_c = pi_c\n",
    "\n",
    "                for each feature j:\n",
    "\n",
    "                    x_new_j = x_new[:,j]\n",
    "                    prob_c *= p(x_new_j | theta_jc)\n",
    "\n",
    "            return class c with the largest prob_c\n",
    "\n",
    "        -In this pseudocode, the input of the predict function is a single data point.\n",
    "        -In the skeleton code, the input of the predict function is a matrix with multiple data points. The\n",
    "        function should predict the labels for all data points.\n",
    "        \"\"\"\n",
    "        # You should use matrix operations rather than loops. In general, loops over classes or features are OK, but loops over the rows of data are not a good idea.\n",
    "\n",
    "        log_lik_per_class = []\n",
    "\n",
    "        for classindex in range(class_len):\n",
    "            # log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^j| ğ‘¦ =ğ‘,ğœƒğ‘—ğ‘)) for all features j\n",
    "            logs_features = []\n",
    "            for featureindex in range(n_features):\n",
    "                theta_class_feature = self.thetas_class_feature[classindex][featureindex]\n",
    "                logs_features.append(theta_class_feature.get_log_probability(X[:, featureindex]))\n",
    "\n",
    "            # sum over features j = âˆ‘ğ‘—(log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^j| ğ‘¦ =ğ‘,ğœƒğ‘—ğ‘)))\n",
    "            log_lik_per_class.append(np.sum(logs_features, axis=0))#rows\n",
    "\n",
    "        log_lik_per_class = np.array(log_lik_per_class).T\n",
    "        joint_log_likelihood = self.log_class_prior + log_lik_per_class # log(ğœ‹ğ‘)+âˆ‘ğ‘—(log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤^j| ğ‘¦ =ğ‘,ğœƒğ‘—ğ‘)))\n",
    "\n",
    "        #Choose the class with largest probability (no need to compute the denominator -log(ğ‘(ğ‘¥ğ‘›ğ‘’ğ‘¤|ğœ½,ğ…)) as it is same for all classes)\n",
    "        indices = np.argmax(joint_log_likelihood, axis=1)#columns\n",
    "        return self.classes[indices]\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**\n",
    "\n",
    "We will use the code below to check your code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:04.663947Z",
     "start_time": "2025-11-22T15:24:03.948172Z"
    }
   },
   "source": [
    "# All features of the iris dataset are continuous.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "\n",
    "nbc_iris = NBC(feature_types=['r', 'r', 'r', 'r'])\n",
    "nbc_iris.fit(Xtrain, ytrain)\n",
    "yhat = nbc_iris.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "\n",
    "print(\"Accuracy:\", test_accuracy) # should be larger than 90%\n",
    "print(yhat)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:04.700724Z",
     "start_time": "2025-11-22T15:24:04.689536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "sk_nbG = GaussianNB()\n",
    "sk_nbG.fit(Xtrain, ytrain)\n",
    "yhat_sklearnG = sk_nbG.predict(Xtest)\n",
    "acc_sklearnG = np.mean(yhat_sklearnG == ytest)\n",
    "\n",
    "print(\"Accuracy:\", acc_sklearnG)\n",
    "print(yhat_sklearnG)\n",
    "\n",
    "if np.array_equal(yhat,yhat_sklearnG):\n",
    "    print(\"same\")\n",
    "else:\n",
    "    print(\"different\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "same\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:04.769910Z",
     "start_time": "2025-11-22T15:24:04.760921Z"
    }
   },
   "source": [
    "# All features of this dataset are binary\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../datasets//binary_test.csv', header=None)\n",
    "data = data.to_numpy()\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "\n",
    "nbc = NBC(feature_types=['b'] * 16)\n",
    "nbc.fit(Xtrain, ytrain)\n",
    "yhat = nbc.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "\n",
    "print(\"Accuracy:\", test_accuracy) # should be larger than 85%\n",
    "print(yhat)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8723404255319149\n",
      "[1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 1 1 0]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:04.819905Z",
     "start_time": "2025-11-22T15:24:04.811860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "sk_nbB = BernoulliNB(alpha=1.0)\n",
    "sk_nbB.fit(Xtrain, ytrain)\n",
    "yhat_sklearnB = sk_nbB.predict(Xtest)\n",
    "acc_sklearnB = np.mean(yhat_sklearnB == ytest)\n",
    "\n",
    "print(\"Accuracy:\", acc_sklearnB)\n",
    "print(yhat_sklearnB)\n",
    "\n",
    "if np.array_equal(yhat,yhat_sklearnB):\n",
    "    print(\"same\")\n",
    "else:\n",
    "    print(\"different\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8723404255319149\n",
      "[1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 1 1 0]\n",
      "same\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:04.878095Z",
     "start_time": "2025-11-22T15:24:04.870679Z"
    }
   },
   "source": [
    "# All features of this dataset are categorical (bonus task)\n",
    "\n",
    "data = pd.read_csv('../datasets/categorical_test.csv', header=None)\n",
    "data = data.to_numpy()\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "\n",
    "nbc = NBC(feature_types=['c'] * 9)\n",
    "nbc.fit(Xtrain, ytrain)\n",
    "yhat = nbc.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "\n",
    "print(\"Accuracy:\", test_accuracy) # should be larger than 65%\n",
    "print(yhat)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6896551724137931\n",
      "[0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:04.941291Z",
     "start_time": "2025-11-22T15:24:04.931278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#scikit-learn CategoricalNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "sk_nbC = CategoricalNB(alpha=1.0)\n",
    "sk_nbC.fit(Xtrain, ytrain)\n",
    "yhat_sklearnC = sk_nbC.predict(Xtest)\n",
    "acc_sklearnC = np.mean(yhat_sklearnC == ytest)\n",
    "\n",
    "print(\"Accuracy:\", acc_sklearnC)\n",
    "print(yhat_sklearnC)\n",
    "\n",
    "if np.array_equal(yhat,yhat_sklearnC):\n",
    "    print(\"same\")\n",
    "else:\n",
    "    print(\"different\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6896551724137931\n",
      "[0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "same\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, you should use the implementation in scikit-learn. Add the following\n",
    "line to import the LR model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:05.155068Z",
     "start_time": "2025-11-22T15:24:04.990068Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the scikit-learn documentation for the Logistic Regression model:\n",
    "- http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing NBC and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "The experiment is to compare the classification error of the NBC and LR trained on increasingly larger training datasets. \n",
    "Since the datasets are so small, you should do this multiple times and\n",
    "average the classification error. One run should look as follows:\n",
    "- Shuffle the data, put 20% aside for testing.\n",
    "    \n",
    "    ```N, D = X.shape\n",
    "    Ntrain = int(0.8 * N)\n",
    "    shuffler = np.random.permutation(N)\n",
    "    Xtrain = X[shuffler[:Ntrain]]\n",
    "    ytrain = y[shuffler[:Ntrain]]\n",
    "    Xtest = X[shuffler[Ntrain:]]\n",
    "    ytest = y[shuffler[Ntrain:]]\n",
    "    \n",
    "    ```  \n",
    "\n",
    "\n",
    "- Train the classifiers with increasingly more data. For example, we can train classifiers with 10%, 20%, ..., 100% of the training data. For each case store the classification errors on the test set of the classifiers.\n",
    "\n",
    "You may want to repeat this with at least 200 random permutations (possibly as large as 1000)\n",
    "to average out the test error across the runs. In the end, you will get average test errors as a function of the size of the training data. \n",
    "We have written for you the function for making the plots for the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:05.170831Z",
     "start_time": "2025-11-22T15:24:05.168060Z"
    }
   },
   "source": [
    "# inputs:\n",
    "#   nbc: Naive Bayes Classifier\n",
    "#   lr: Logistic Regression Classifier\n",
    "#   X, y: data\n",
    "#   num_runs: we need repeat num_runs times and store average results\n",
    "#   num_splits: we want to compare the two models on increasingly larger training sets.\n",
    "#               num_splits defines the number of increasing steps. \n",
    "# outputs:\n",
    "#   the arrays of the test errors across the runs of the two classifiers \n",
    "def compareNBCvsLR(nbc, lr, X, y, num_runs=200, num_splits=10):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    tst_errs_nbc = []\n",
    "    tst_errs_lr = []\n",
    "    \n",
    "    return tst_errs_nbc, tst_errs_lr\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:05.222971Z",
     "start_time": "2025-11-22T15:24:05.219178Z"
    }
   },
   "source": [
    "def makePlot(nbc_perf, lr_perf, title=None, num_splits=10):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.set_xlabel('Percent of training data used', fontsize=20)\n",
    "    ax.set_ylabel('Classification Error', fontsize=20)\n",
    "    if title is not None: ax.set_title(title, fontsize=25)\n",
    "\n",
    "    xaxis_scale = [(i + 1) * (100/num_splits) for i in range(num_splits)]\n",
    "    plt.plot(xaxis_scale, nbc_perf, label='Naive Bayes')\n",
    "    plt.plot(xaxis_scale, lr_perf, label='Logistic Regression', linestyle='dashed')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=20)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks: For each dataset,\n",
    "1. Prepare the data for the two classifiers, e.g., handle missing values and the categorical data. When you handle the categorical data, you should check whether the data is ordinal or not. If the data is ordinal, you should encode the data as integers. If the data is not ordinal, you should encode the data as one-hot vectors.\n",
    "2. Show the first 5 rows of the prepared data\n",
    "3. Compare the two classifiers on the dataset and generate the plots\n",
    "\n",
    "The grading will be based on whether the data is correctly prepared and the plots are generated without errors. The grading will not be based on the performance of the classifiers and whether the plots are the same as in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Iris Dataset**\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:05.272431Z",
     "start_time": "2025-11-22T15:24:05.268278Z"
    }
   },
   "source": [
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Voting Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/congressional+voting+records\n",
    "\n",
    "The logistic regression line meets the naive bayes line early in the plot. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T15:24:05.707600Z",
     "start_time": "2025-11-22T15:24:05.328667Z"
    }
   },
   "source": [
    "# load the dataset\n",
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "voting = pd.read_csv('./datasets/voting.csv')\n",
    "\n",
    "voting.info()\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/voting.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# load the dataset\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# TODO: insert your code for experiments\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m###################################################\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m##### YOUR CODE STARTS HERE #######################\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m###################################################\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m voting \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./datasets/voting.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m voting\u001B[38;5;241m.\u001B[39minfo()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m###################################################\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m##### YOUR CODE ENDS HERE #########################\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m###################################################\u001B[39;00m\n",
      "File \u001B[0;32m~/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/repositories/FoDS_Practicals/.venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './datasets/voting.csv'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 3: Breast Cancer Dataset (Bonus Tasks)**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/breast+cancer\n",
    "\n",
    "The dataset has continues, binary and categorical features. It also has missing values.\n",
    "\n",
    "Hints: You can precompute the size of the domains of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load the dataset\n",
    "# TODO: insert your code for experiments\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "cancer = pd.read_csv('./datasets/breast-cancer.csv')\n",
    "\n",
    "cancer.info()\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
